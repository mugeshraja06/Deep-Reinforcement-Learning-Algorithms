{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning Algorithms  -Tensorflow",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM/fckEL2hfDTi138ZqtA9g"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ6AXX_1lHHL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "\n",
        "# !apt-get install python-opengl -y\n",
        "\n",
        "# !apt install xvfb -y\n",
        "\n",
        "# !pip install pyvirtualdisplay\n",
        "\n",
        "# !pip install piglet\n",
        "\n",
        "# from pyvirtualdisplay import Display\n",
        "# Display().start()\n",
        "\n",
        "import gym\n",
        "# from IPython import display\n",
        "# import matplotlib.pyplot as plt\n",
        "# %matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install swig \n",
        "# ! pip install box2d-py\n",
        "\n",
        "! pip install box2d-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ8ZBImWzNyf",
        "outputId": "cfc80670-2ded-4c12-81ce-25664ff7e7ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting box2d-py\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 23.0 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 26.8 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 112 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 448 kB 25.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curve(x, scores, figure_file):\n",
        "    running_avg = np.zeros(len(scores))\n",
        "    for i in range(len(running_avg)):\n",
        "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
        "    plt.plot(x, running_avg)\n",
        "    plt.title('Running average of previous 100 scores')\n",
        "    plt.savefig(figure_file)"
      ],
      "metadata": {
        "id": "R3sr6QQzXvmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actor Critic"
      ],
      "metadata": {
        "id": "rjB--1H8XwDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCriticNetwork(keras.Model):\n",
        "  def __init__(self,n_actions, fc1_dims=1024,fc2_dims=512, name='actor_critic', chkpt_dir='tmp/actor_critic'):\n",
        "    super(ActorCriticNetwork, self).__init__()\n",
        "    self.fc1_dims =fc1_dims\n",
        "    self.fc2_dims =fc2_dims\n",
        "    self.n_actions =n_actions\n",
        "    self.model_name =name\n",
        "    self.checkpoint_dir =chkpt_dir\n",
        "    self.checkpoint_file=os.path.join(self.checkpoint_dir , name+'_ac')\n",
        "    \n",
        "    self.fc1 =Dense(self.fc1_dims, activation='relu')\n",
        "    self.fc2 =Dense(self.fc1_dims, activation='relu')\n",
        "    self.v =Dense(1, activation=None)\n",
        "    self.pi =Dense(n_actions, activation='softmax')\n",
        "\n",
        "  def call(self, state):\n",
        "    value =self.fc1(state)\n",
        "    value =self.fc2(value)\n",
        "\n",
        "    v=self.v(value)\n",
        "    pi=self.pi(value)\n",
        "\n",
        "    return v, pi"
      ],
      "metadata": {
        "id": "S_18_qbLlSnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Agent:\n",
        "  def __init__(self, alpha=0.0003, gamma=0.99, n_actions=2):\n",
        "    self.gamma =gamma\n",
        "    self.n_actions =n_actions\n",
        "    self.action =None\n",
        "    self.action_space =[i for i in range(self.n_actions)]\n",
        "    self.actor_critic =ActorCriticNetwork(n_actions =n_actions)\n",
        "    self.actor_critic.compile(optimizer =Adam(learning_rate= alpha))\n",
        "\n",
        "  def choose_action(self, observation):\n",
        "    state =tensorflow.convert_to_tensor([observation])\n",
        "    _, probs =self.actor_critic(state)\n",
        "\n",
        "    action_probalities =tfp.distributions.Categorical(probs=probs)\n",
        "    action=action_probalities.sample()\n",
        "    self.action =action\n",
        "\n",
        "    return action.numpy()[0]\n",
        "\n",
        "  def save_models(self):\n",
        "    print(\"--saving models--\")\n",
        "    self.actor_critic.save_weights(self.actor_critic.checkpoint_file)\n",
        "\n",
        "  def load_models(self):\n",
        "    print(\"--loading models--\")\n",
        "    self.actor_critic.load_weights(self.actor_critic.checkpoint_file)  \n",
        "\n",
        "  def learn(self, state, reward, state_, done):\n",
        "    state =tensorflow.convert_to_tensor([state], dtype=tensorflow.float32)\n",
        "    state_ =tensorflow.convert_to_tensor([state_], dtype=tensorflow.float32)\n",
        "    reward =tensorflow.convert_to_tensor([reward], dtype=tensorflow.float32)\n",
        "\n",
        "    with tensorflow.GradientTape(persistent=True) as tape:\n",
        "      state_value, probs =self.actor_critic(state)\n",
        "      state_value_, _ =self.actor_critic(state_)\n",
        "      state_value =tensorflow.squeeze(state_value)\n",
        "      state_value_ =tensorflow.squeeze(state_value_)\n",
        "\n",
        "      action_probs =tfp.distributions.Categorical(probs=probs)\n",
        "      log_prob =action_probs.log_prob(self.action)\n",
        "\n",
        "      delta =reward + self.gamma*state_value_*(1-int(done)) - state_value\n",
        "      actor_loss = -log_prob *delta\n",
        "      critic_loss =delta *2\n",
        "\n",
        "      total_loss =actor_loss + critic_loss\n",
        "\n",
        "    gradient =tape.gradient(total_loss, self.actor_critic.trainable_variables)\n",
        "    self.actor_critic.optimizer.apply_gradients(zip(gradient, self.actor_critic.trainable_variables))\n",
        "  "
      ],
      "metadata": {
        "id": "fu1a0Rx1lSpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ =='__main__':\n",
        "  env = gym.make('CartPole-v0')\n",
        "  agent= Agent(alpha=1e-5, n_actions=env.action_space.n)\n",
        "  n_games =1800\n",
        "\n",
        "  filename = 'cartploe.png'\n",
        "  figure_file ='/content/tmp/' +filename\n",
        "\n",
        "  best_score =env.reward_range[0]\n",
        "  score_history =[]\n",
        "  load_checkpoint =False\n",
        "  \n",
        "  if load_checkpoint:\n",
        "    agent.load_model()\n",
        "\n",
        "  for i in range(n_games):\n",
        "    observation = env.reset()\n",
        "    done =False\n",
        "    score =0\n",
        "    while not done:\n",
        "      action =agent.choose_action(observation)\n",
        "      observation_, reward, done, info =env.step(action)\n",
        "      score +=reward\n",
        "\n",
        "      if not load_checkpoint:\n",
        "        agent.learn(observation, reward, observation_, done)\n",
        "      observation =observation_\n",
        "    score_history.append(score)\n",
        "    avg_score =np.mean(score_history[-100:])\n",
        "\n",
        "    if avg_score >best_score:\n",
        "      best_score =avg_score\n",
        "      if not load_checkpoint:\n",
        "        agent.save_models()\n",
        "\n",
        "  x =[i+1 for i in range(n_games)] \n",
        "  plot_learning_curve(x, score_history, figure_file)      \n"
      ],
      "metadata": {
        "id": "aFyhSam5lStL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "aafce5eb-90d9-45f2-93cc-63f6427d0d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--saving models--\n",
            "--saving models--\n",
            "--saving models--\n",
            "--saving models--\n",
            "--saving models--\n",
            "--saving models--\n",
            "--saving models--\n",
            "--saving models--\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# env = gym.make('CartPole-v0')\n",
        "# env.reset()\n",
        "# img = plt.imshow(env.render('rgb_array')) # only call this once\n",
        "# for _ in range(40):\n",
        "#     img.set_data(env.render('rgb_array')) # just update the data\n",
        "#     display.display(plt.gcf())\n",
        "#     display.clear_output(wait=True)\n",
        "#     action = env.action_space.sample()\n",
        "#     env.step(action)"
      ],
      "metadata": {
        "id": "etl2-CgdlSzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Deterministic Policy Gradients (DDPG)"
      ],
      "metadata": {
        "id": "N2BSTqEdX5Cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "  def __init__(self, max_size, input_shape, n_actions):\n",
        "    self.mem_size =max_size\n",
        "    self.mem_cntr =0\n",
        "    self.state_memory =np.zeros((self.mem_size, *input_shape))\n",
        "    self.new_state_memory =np.zeros((self.mem_size, *input_shape))\n",
        "    self.action_memory =np.zeros((self.mem_size, n_actions))\n",
        "    self.reward_memory =np.zeros(self.mem_size)\n",
        "    self.terminal_memory =np.zeros(self.mem_size, dtype=np.bool)\n",
        "\n",
        "  def store_transition(self, state, action, reward, new_state, done):\n",
        "    index =self.mem_cntr % self.mem_size\n",
        "\n",
        "    self.state_memory[index] =state\n",
        "    self.new_state_memory[index] =new_state\n",
        "    self.action_memory[index] =action\n",
        "    self.reward_memory[index] =reward\n",
        "    self.terminal_memory[index] =done\n",
        "\n",
        "    self.mem_cntr +=1\n",
        "\n",
        "  def sample_buffer(self, batch_size):\n",
        "    max_mem =min(self.mem_cntr, self.mem_size)\n",
        "    batch =np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "    states =self.state_memory[batch]\n",
        "    new_state =self.new_state_memory[batch]\n",
        "    actions =self.action_memory[batch]\n",
        "    rewards =self.reward_memory[batch]\n",
        "    dones =self.terminal_memory[batch]\n",
        "\n",
        "    return states, actions, rewards, new_state, dones"
      ],
      "metadata": {
        "id": "vaa_KefFlS0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CriticNetwork(keras.Model):\n",
        "    def __init__(self, fc1_dims=512, fc2_dims=512, name='critic', chkpt_dir='tmp/ddpg'):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "\n",
        "        self.model_name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_ddpg.h5')\n",
        "\n",
        "        self.fc1 = Dense(self.fc1_dims, activation='relu')\n",
        "        self.fc2 = Dense(self.fc2_dims, activation='relu')\n",
        "        self.q = Dense(1, activation=None)\n",
        "\n",
        "    def call(self, state, action):\n",
        "        action_value = self.fc1(tensorflow.concat([state, action], axis=1))\n",
        "        action_value = self.fc2(action_value)\n",
        "\n",
        "        q = self.q(action_value)\n",
        "\n",
        "        return q\n",
        "\n",
        "class ActorNetwork(keras.Model):\n",
        "    def __init__(self, fc1_dims=512, fc2_dims=512, n_actions=2, name='actor', chkpt_dir='tmp/ddpg'):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        self.model_name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_ddpg.h5')\n",
        "\n",
        "        self.fc1 = Dense(self.fc1_dims, activation='relu')\n",
        "        self.fc2 = Dense(self.fc2_dims, activation='relu')\n",
        "        self.mu = Dense(self.n_actions, activation='tanh')\n",
        "\n",
        "    def call(self, state):\n",
        "        prob = self.fc1(state)\n",
        "        prob = self.fc2(prob)\n",
        "\n",
        "        mu = self.mu(prob)\n",
        "\n",
        "        return mu"
      ],
      "metadata": {
        "id": "vEHjLOsZlS4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, input_dims, alpha=0.001, beta=0.002, env=None,gamma=0.99, n_actions=2, max_size=1000000, tau=0.005,\n",
        "                 fc1=400, fc2=300, batch_size=64, noise=0.1):\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
        "        self.batch_size = batch_size\n",
        "        self.n_actions = n_actions\n",
        "        self.noise = noise\n",
        "        self.max_action = env.action_space.high[0]\n",
        "        self.min_action = env.action_space.low[0]\n",
        "\n",
        "        self.actor = ActorNetwork(n_actions=n_actions, name='actor')\n",
        "        self.critic = CriticNetwork(name='critic')\n",
        "        self.target_actor = ActorNetwork(n_actions=n_actions,name='target_actor')\n",
        "        self.target_critic = CriticNetwork(name='target_critic')\n",
        "\n",
        "        self.actor.compile(optimizer=Adam(learning_rate=alpha))\n",
        "        self.critic.compile(optimizer=Adam(learning_rate=beta))\n",
        "        self.target_actor.compile(optimizer=Adam(learning_rate=alpha))\n",
        "        self.target_critic.compile(optimizer=Adam(learning_rate=beta))\n",
        "\n",
        "        self.update_network_parameters(tau=1)\n",
        "\n",
        "    def update_network_parameters(self, tau=None):\n",
        "        if tau is None:\n",
        "            tau = self.tau\n",
        "\n",
        "        weights = []\n",
        "        targets = self.target_actor.weights\n",
        "        for i, weight in enumerate(self.actor.weights):\n",
        "            weights.append(weight * tau + targets[i]*(1-tau))\n",
        "        self.target_actor.set_weights(weights)\n",
        "\n",
        "        weights = []\n",
        "        targets = self.target_critic.weights\n",
        "        for i, weight in enumerate(self.critic.weights):\n",
        "            weights.append(weight * tau + targets[i]*(1-tau))\n",
        "        self.target_critic.set_weights(weights)\n",
        "\n",
        "    def remember(self, state, action, reward, new_state, done):\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    def save_models(self):\n",
        "        print('... saving models ...')\n",
        "        self.actor.save_weights(self.actor.checkpoint_file)\n",
        "        self.target_actor.save_weights(self.target_actor.checkpoint_file)\n",
        "        self.critic.save_weights(self.critic.checkpoint_file)\n",
        "        self.target_critic.save_weights(self.target_critic.checkpoint_file)\n",
        "\n",
        "    def load_models(self):\n",
        "        print('... loading models ...')\n",
        "        self.actor.load_weights(self.actor.checkpoint_file)\n",
        "        self.target_actor.load_weights(self.target_actor.checkpoint_file)\n",
        "        self.critic.load_weights(self.critic.checkpoint_file)\n",
        "        self.target_critic.load_weights(self.target_critic.checkpoint_file)\n",
        "\n",
        "    def choose_action(self, observation, evaluate=False):\n",
        "        state = tensorflow.convert_to_tensor([observation], dtype=tensorflow.float32)\n",
        "        actions = self.actor(state)\n",
        "        if not evaluate:\n",
        "            actions += tensorflow.random.normal(shape=[self.n_actions],\n",
        "                                        mean=0.0, stddev=self.noise)\n",
        "        # note that if the env has an action > 1, we have to multiply by\n",
        "        # max action at some point\n",
        "        actions = tensorflow.clip_by_value(actions, self.min_action, self.max_action)\n",
        "\n",
        "        return actions[0]\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.mem_cntr < self.batch_size:\n",
        "            return\n",
        "\n",
        "        state, action, reward, new_state, done = \\\n",
        "            self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "        states = tensorflow.convert_to_tensor(state, dtype=tensorflow.float32)\n",
        "        new_state = tensorflow.convert_to_tensor(new_state, dtype=tensorflow.float32)\n",
        "        rewards = tensorflow.convert_to_tensor(reward, dtype=tensorflow.float32)\n",
        "        actions = tensorflow.convert_to_tensor(action, dtype=tensorflow.float32)\n",
        "\n",
        "        with tensorflow.GradientTape() as tape:\n",
        "            target_actions = self.target_actor(new_state)\n",
        "            critic_value_ = tensorflow.squeeze(self.target_critic(\n",
        "                                new_state, target_actions), 1)\n",
        "            critic_value = tensorflow.squeeze(self.critic(states, actions), 1)\n",
        "            target = rewards + self.gamma*critic_value_*(1-done)\n",
        "            critic_loss = keras.losses.MSE(target, critic_value)\n",
        "\n",
        "        critic_network_gradient = tape.gradient(critic_loss,\n",
        "                                                self.critic.trainable_variables)\n",
        "        self.critic.optimizer.apply_gradients(zip(\n",
        "            critic_network_gradient, self.critic.trainable_variables))\n",
        "\n",
        "        with tensorflow.GradientTape() as tape:\n",
        "            new_policy_actions = self.actor(states)\n",
        "            actor_loss = -self.critic(states, new_policy_actions)\n",
        "            actor_loss = tensorflow.math.reduce_mean(actor_loss)\n",
        "\n",
        "        actor_network_gradient = tape.gradient(actor_loss,\n",
        "                                               self.actor.trainable_variables)\n",
        "        self.actor.optimizer.apply_gradients(zip(\n",
        "            actor_network_gradient, self.actor.trainable_variables))\n",
        "\n",
        "        self.update_network_parameters()"
      ],
      "metadata": {
        "id": "t0IRRDsvlS-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    env = gym.make('Pendulum-v0')\n",
        "    agent = Agent(input_dims=env.observation_space.shape, env=env,\n",
        "            n_actions=env.action_space.shape[0])\n",
        "    n_games = 250\n",
        "\n",
        "    figure_file = '/content/tmp/pendulum.png'\n",
        "\n",
        "    best_score = env.reward_range[0]\n",
        "    score_history = []\n",
        "    load_checkpoint = False\n",
        "\n",
        "    if load_checkpoint:\n",
        "        n_steps = 0\n",
        "        while n_steps <= agent.batch_size:\n",
        "            observation = env.reset()\n",
        "            action = env.action_space.sample()\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            agent.remember(observation, action, reward, observation_, done)\n",
        "            n_steps += 1\n",
        "        agent.learn()\n",
        "        agent.load_models()\n",
        "        evaluate = True\n",
        "    else:\n",
        "        evaluate = False\n",
        "\n",
        "    for i in range(n_games):\n",
        "        observation = env.reset()\n",
        "        done = False\n",
        "        score = 0\n",
        "        while not done:\n",
        "            action = agent.choose_action(observation, evaluate)\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            agent.remember(observation, action, reward, observation_, done)\n",
        "            if not load_checkpoint:\n",
        "                agent.learn()\n",
        "            observation = observation_\n",
        "\n",
        "        score_history.append(score)\n",
        "        avg_score = np.mean(score_history[-100:])\n",
        "\n",
        "        if avg_score > best_score:\n",
        "            best_score = avg_score\n",
        "            if not load_checkpoint:\n",
        "                agent.save_models()\n",
        "\n",
        "        print('episode ', i, 'score %.1f' % score, 'avg score %.1f' % avg_score)\n",
        "\n",
        "    if not load_checkpoint:\n",
        "        x = [i+1 for i in range(n_games)]\n",
        "        plot_learning_curve(x, score_history, figure_file)"
      ],
      "metadata": {
        "id": "lV_4AZoylTBr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "18183cd3-d2a0-4622-a7ee-56fbc9f113da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... saving models ...\n",
            "episode  0 score -1247.8 avg score -1247.8\n",
            "episode  1 score -1790.2 avg score -1519.0\n",
            "episode  2 score -1738.2 avg score -1592.1\n",
            "episode  3 score -1755.9 avg score -1633.0\n",
            "episode  4 score -1567.8 avg score -1620.0\n",
            "episode  5 score -1109.3 avg score -1534.9\n",
            "episode  6 score -1188.6 avg score -1485.4\n",
            "episode  7 score -1383.2 avg score -1472.6\n",
            "episode  8 score -1210.2 avg score -1443.5\n",
            "episode  9 score -1172.0 avg score -1416.3\n",
            "episode  10 score -1663.2 avg score -1438.8\n",
            "episode  11 score -1190.2 avg score -1418.0\n",
            "episode  12 score -1045.7 avg score -1389.4\n",
            "episode  13 score -1031.6 avg score -1363.8\n",
            "episode  14 score -1137.1 avg score -1348.7\n",
            "episode  15 score -777.1 avg score -1313.0\n",
            "episode  16 score -785.2 avg score -1282.0\n",
            "episode  17 score -1108.8 avg score -1272.3\n",
            "episode  18 score -1113.5 avg score -1264.0\n",
            "episode  19 score -1038.6 avg score -1252.7\n",
            "episode  20 score -1356.1 avg score -1257.6\n",
            "... saving models ...\n",
            "episode  21 score -890.5 avg score -1240.9\n",
            "... saving models ...\n",
            "episode  22 score -760.0 avg score -1220.0\n",
            "... saving models ...\n",
            "episode  23 score -510.7 avg score -1190.5\n",
            "... saving models ...\n",
            "episode  24 score -498.8 avg score -1162.8\n",
            "... saving models ...\n",
            "episode  25 score -563.1 avg score -1139.7\n",
            "... saving models ...\n",
            "episode  26 score -493.7 avg score -1115.8\n",
            "... saving models ...\n",
            "episode  27 score -255.7 avg score -1085.1\n",
            "... saving models ...\n",
            "episode  28 score -3.0 avg score -1047.8\n",
            "... saving models ...\n",
            "episode  29 score -2.9 avg score -1013.0\n",
            "... saving models ...\n",
            "episode  30 score -608.7 avg score -999.9\n",
            "... saving models ...\n",
            "episode  31 score -373.7 avg score -980.3\n",
            "... saving models ...\n",
            "episode  32 score -373.0 avg score -961.9\n",
            "... saving models ...\n",
            "episode  33 score -126.7 avg score -937.4\n",
            "... saving models ...\n",
            "episode  34 score -493.9 avg score -924.7\n",
            "... saving models ...\n",
            "episode  35 score -360.9 avg score -909.0\n",
            "... saving models ...\n",
            "episode  36 score -632.8 avg score -901.6\n",
            "... saving models ...\n",
            "episode  37 score -509.8 avg score -891.3\n",
            "... saving models ...\n",
            "episode  38 score -127.2 avg score -871.7\n",
            "... saving models ...\n",
            "episode  39 score -125.1 avg score -853.0\n",
            "... saving models ...\n",
            "episode  40 score -121.2 avg score -835.2\n",
            "... saving models ...\n",
            "episode  41 score -124.1 avg score -818.2\n",
            "... saving models ...\n",
            "episode  42 score -497.4 avg score -810.8\n",
            "... saving models ...\n",
            "episode  43 score -123.6 avg score -795.2\n",
            "... saving models ...\n",
            "episode  44 score -124.7 avg score -780.3\n",
            "... saving models ...\n",
            "episode  45 score -504.7 avg score -774.3\n",
            "... saving models ...\n",
            "episode  46 score -121.9 avg score -760.4\n",
            "... saving models ...\n",
            "episode  47 score -628.4 avg score -757.6\n",
            "... saving models ...\n",
            "episode  48 score -125.9 avg score -744.7\n",
            "... saving models ...\n",
            "episode  49 score -243.5 avg score -734.7\n",
            "... saving models ...\n",
            "episode  50 score -127.5 avg score -722.8\n",
            "... saving models ...\n",
            "episode  51 score -483.7 avg score -718.2\n",
            "... saving models ...\n",
            "episode  52 score -126.6 avg score -707.0\n",
            "... saving models ...\n",
            "episode  53 score -483.2 avg score -702.9\n",
            "... saving models ...\n",
            "episode  54 score -126.8 avg score -692.4\n",
            "... saving models ...\n",
            "episode  55 score -123.8 avg score -682.3\n",
            "... saving models ...\n",
            "episode  56 score -236.4 avg score -674.5\n",
            "... saving models ...\n",
            "episode  57 score -122.9 avg score -664.9\n",
            "... saving models ...\n",
            "episode  58 score -468.4 avg score -661.6\n",
            "... saving models ...\n",
            "episode  59 score -476.1 avg score -658.5\n",
            "... saving models ...\n",
            "episode  60 score -122.4 avg score -649.7\n",
            "... saving models ...\n",
            "episode  61 score -366.9 avg score -645.2\n",
            "... saving models ...\n",
            "episode  62 score -243.7 avg score -638.8\n",
            "... saving models ...\n",
            "episode  63 score -242.2 avg score -632.6\n",
            "... saving models ...\n",
            "episode  64 score -121.4 avg score -624.7\n",
            "... saving models ...\n",
            "episode  65 score -125.1 avg score -617.2\n",
            "... saving models ...\n",
            "episode  66 score -528.6 avg score -615.8\n",
            "... saving models ...\n",
            "episode  67 score -122.9 avg score -608.6\n",
            "... saving models ...\n",
            "episode  68 score -124.4 avg score -601.6\n",
            "... saving models ...\n",
            "episode  69 score -236.5 avg score -596.4\n",
            "... saving models ...\n",
            "episode  70 score -473.2 avg score -594.6\n",
            "... saving models ...\n",
            "episode  71 score -476.5 avg score -593.0\n",
            "... saving models ...\n",
            "episode  72 score -123.2 avg score -586.6\n",
            "... saving models ...\n",
            "episode  73 score -0.5 avg score -578.6\n",
            "... saving models ...\n",
            "episode  74 score -119.4 avg score -572.5\n",
            "... saving models ...\n",
            "episode  75 score -357.9 avg score -569.7\n",
            "... saving models ...\n",
            "episode  76 score -127.3 avg score -563.9\n",
            "episode  77 score -588.3 avg score -564.3\n",
            "... saving models ...\n",
            "episode  78 score -0.2 avg score -557.1\n",
            "... saving models ...\n",
            "episode  79 score -125.8 avg score -551.7\n",
            "... saving models ...\n",
            "episode  80 score -358.5 avg score -549.3\n",
            "... saving models ...\n",
            "episode  81 score -236.3 avg score -545.5\n",
            "... saving models ...\n",
            "episode  82 score -241.5 avg score -541.9\n",
            "... saving models ...\n",
            "episode  83 score -126.9 avg score -536.9\n",
            "... saving models ...\n",
            "episode  84 score -121.8 avg score -532.0\n",
            "... saving models ...\n",
            "episode  85 score -121.9 avg score -527.3\n",
            "... saving models ...\n",
            "episode  86 score -124.9 avg score -522.6\n",
            "episode  87 score -623.8 avg score -523.8\n",
            "... saving models ...\n",
            "episode  88 score -123.0 avg score -519.3\n",
            "episode  89 score -605.2 avg score -520.2\n",
            "episode  90 score -488.3 avg score -519.9\n",
            "... saving models ...\n",
            "episode  91 score -352.7 avg score -518.1\n",
            "... saving models ...\n",
            "episode  92 score -124.2 avg score -513.8\n",
            "... saving models ...\n",
            "episode  93 score -359.0 avg score -512.2\n",
            "... saving models ...\n",
            "episode  94 score -125.1 avg score -508.1\n",
            "... saving models ...\n",
            "episode  95 score -122.3 avg score -504.1\n",
            "... saving models ...\n",
            "episode  96 score -122.1 avg score -500.2\n",
            "... saving models ...\n",
            "episode  97 score -125.0 avg score -496.3\n",
            "... saving models ...\n",
            "episode  98 score -241.2 avg score -493.8\n",
            "... saving models ...\n",
            "episode  99 score -122.2 avg score -490.0\n",
            "... saving models ...\n",
            "episode  100 score -238.0 avg score -479.9\n",
            "... saving models ...\n",
            "episode  101 score -0.4 avg score -462.0\n",
            "... saving models ...\n",
            "episode  102 score -351.6 avg score -448.2\n",
            "... saving models ...\n",
            "episode  103 score -123.9 avg score -431.9\n",
            "... saving models ...\n",
            "episode  104 score -233.4 avg score -418.5\n",
            "... saving models ...\n",
            "episode  105 score -477.3 avg score -412.2\n",
            "... saving models ...\n",
            "episode  106 score -120.6 avg score -401.5\n",
            "... saving models ...\n",
            "episode  107 score -121.4 avg score -388.9\n",
            "... saving models ...\n",
            "episode  108 score -240.0 avg score -379.2\n",
            "... saving models ...\n",
            "episode  109 score -512.8 avg score -372.6\n",
            "... saving models ...\n",
            "episode  110 score -122.7 avg score -357.2\n",
            "... saving models ...\n",
            "episode  111 score -362.2 avg score -348.9\n",
            "... saving models ...\n",
            "episode  112 score -236.3 avg score -340.8\n",
            "... saving models ...\n",
            "episode  113 score -238.0 avg score -332.9\n",
            "... saving models ...\n",
            "episode  114 score -243.7 avg score -323.9\n",
            "... saving models ...\n",
            "episode  115 score -122.3 avg score -317.4\n",
            "... saving models ...\n",
            "episode  116 score -606.2 avg score -315.6\n",
            "... saving models ...\n",
            "episode  117 score -361.3 avg score -308.1\n",
            "... saving models ...\n",
            "episode  118 score -124.2 avg score -298.2\n",
            "... saving models ...\n",
            "episode  119 score -242.1 avg score -290.3\n",
            "... saving models ...\n",
            "episode  120 score -126.6 avg score -278.0\n",
            "... saving models ...\n",
            "episode  121 score -1.6 avg score -269.1\n",
            "... saving models ...\n",
            "episode  122 score -122.4 avg score -262.7\n",
            "... saving models ...\n",
            "episode  123 score -493.3 avg score -262.5\n",
            "... saving models ...\n",
            "episode  124 score -480.7 avg score -262.4\n",
            "... saving models ...\n",
            "episode  125 score -241.7 avg score -259.2\n",
            "episode  126 score -612.0 avg score -260.3\n",
            "... saving models ...\n",
            "episode  127 score -123.2 avg score -259.0\n",
            "episode  128 score -234.9 avg score -261.3\n",
            "episode  129 score -358.6 avg score -264.9\n",
            "episode  130 score -242.6 avg score -261.2\n",
            "... saving models ...\n",
            "episode  131 score -125.0 avg score -258.7\n",
            "... saving models ...\n",
            "episode  132 score -0.9 avg score -255.0\n",
            "episode  133 score -520.5 avg score -259.0\n",
            "episode  134 score -119.5 avg score -255.2\n",
            "episode  135 score -356.3 avg score -255.2\n",
            "... saving models ...\n",
            "episode  136 score -599.6 avg score -254.8\n",
            "... saving models ...\n",
            "episode  137 score -350.3 avg score -253.2\n",
            "episode  138 score -556.0 avg score -257.5\n",
            "episode  139 score -236.1 avg score -258.6\n",
            "episode  140 score -119.4 avg score -258.6\n",
            "episode  141 score -122.5 avg score -258.6\n",
            "episode  142 score -125.5 avg score -254.9\n",
            "episode  143 score -125.2 avg score -254.9\n",
            "episode  144 score -241.1 avg score -256.1\n",
            "episode  145 score -238.9 avg score -253.4\n",
            "episode  146 score -236.4 avg score -254.6\n",
            "... saving models ...\n",
            "episode  147 score -242.0 avg score -250.7\n",
            "episode  148 score -245.1 avg score -251.9\n",
            "episode  149 score -359.0 avg score -253.0\n",
            "episode  150 score -241.7 avg score -254.2\n",
            "... saving models ...\n",
            "episode  151 score -0.8 avg score -249.3\n",
            "episode  152 score -480.2 avg score -252.9\n",
            "episode  153 score -596.1 avg score -254.0\n",
            "episode  154 score -355.3 avg score -256.3\n",
            "episode  155 score -488.6 avg score -259.9\n",
            "episode  156 score -362.5 avg score -261.2\n",
            "episode  157 score -234.9 avg score -262.3\n",
            "episode  158 score -580.2 avg score -263.4\n",
            "episode  159 score -605.7 avg score -264.7\n",
            "episode  160 score -125.1 avg score -264.8\n",
            "episode  161 score -611.2 avg score -267.2\n",
            "episode  162 score -119.8 avg score -266.0\n",
            "episode  163 score -124.0 avg score -264.8\n",
            "episode  164 score -126.4 avg score -264.8\n",
            "episode  165 score -124.7 avg score -264.8\n",
            "episode  166 score -127.5 avg score -260.8\n",
            "episode  167 score -119.5 avg score -260.8\n",
            "episode  168 score -120.0 avg score -260.7\n",
            "episode  169 score -238.9 avg score -260.8\n",
            "episode  170 score -567.3 avg score -261.7\n",
            "episode  171 score -243.1 avg score -259.4\n",
            "episode  172 score -122.5 avg score -259.4\n",
            "episode  173 score -237.3 avg score -261.7\n",
            "episode  174 score -500.6 avg score -265.5\n",
            "episode  175 score -124.9 avg score -263.2\n",
            "episode  176 score -244.4 avg score -264.4\n",
            "episode  177 score -0.5 avg score -258.5\n",
            "episode  178 score -122.0 avg score -259.7\n",
            "episode  179 score -366.8 avg score -262.1\n",
            "episode  180 score -512.0 avg score -263.7\n",
            "episode  181 score -118.5 avg score -262.5\n",
            "episode  182 score -477.4 avg score -264.9\n",
            "episode  183 score -122.0 avg score -264.8\n",
            "episode  184 score -607.9 avg score -269.7\n",
            "episode  185 score -123.4 avg score -269.7\n",
            "episode  186 score -124.3 avg score -269.7\n",
            "episode  187 score -355.7 avg score -267.0\n",
            "episode  188 score -126.8 avg score -267.0\n",
            "episode  189 score -354.4 avg score -264.5\n",
            "episode  190 score -121.3 avg score -260.9\n",
            "episode  191 score -244.7 avg score -259.8\n",
            "episode  192 score -354.8 avg score -262.1\n",
            "episode  193 score -122.6 avg score -259.7\n",
            "episode  194 score -123.5 avg score -259.7\n",
            "episode  195 score -118.1 avg score -259.7\n",
            "episode  196 score -629.3 avg score -264.7\n",
            "episode  197 score -599.7 avg score -269.5\n",
            "episode  198 score -123.5 avg score -268.3\n",
            "episode  199 score -117.8 avg score -268.3\n",
            "episode  200 score -124.2 avg score -267.1\n",
            "episode  201 score -361.5 avg score -270.7\n",
            "episode  202 score -121.9 avg score -268.4\n",
            "episode  203 score -240.0 avg score -269.6\n",
            "episode  204 score -545.6 avg score -272.7\n",
            "episode  205 score -125.0 avg score -269.2\n",
            "episode  206 score -0.1 avg score -268.0\n",
            "episode  207 score -122.4 avg score -268.0\n",
            "episode  208 score -479.3 avg score -270.4\n",
            "episode  209 score -482.8 avg score -270.1\n",
            "episode  210 score -124.9 avg score -270.1\n",
            "episode  211 score -127.3 avg score -267.8\n",
            "episode  212 score -124.5 avg score -266.6\n",
            "episode  213 score -350.6 avg score -267.8\n",
            "episode  214 score -126.0 avg score -266.6\n",
            "episode  215 score -124.1 avg score -266.6\n",
            "episode  216 score -513.1 avg score -265.7\n",
            "episode  217 score -239.0 avg score -264.5\n",
            "episode  218 score -1.4 avg score -263.2\n",
            "episode  219 score -124.2 avg score -262.1\n",
            "episode  220 score -122.0 avg score -262.0\n",
            "episode  221 score -120.6 avg score -263.2\n",
            "episode  222 score -123.5 avg score -263.2\n",
            "episode  223 score -122.7 avg score -259.5\n",
            "episode  224 score -362.8 avg score -258.3\n",
            "episode  225 score -240.1 avg score -258.3\n",
            "episode  226 score -122.6 avg score -253.4\n",
            "episode  227 score -125.7 avg score -253.4\n",
            "episode  228 score -615.6 avg score -257.2\n",
            "episode  229 score -126.7 avg score -254.9\n",
            "episode  230 score -119.5 avg score -253.7\n",
            "episode  231 score -123.6 avg score -253.7\n",
            "episode  232 score -503.5 avg score -258.7\n",
            "episode  233 score -128.2 avg score -254.8\n",
            "episode  234 score -637.0 avg score -260.0\n",
            "episode  235 score -674.1 avg score -263.1\n",
            "episode  236 score -474.3 avg score -261.9\n",
            "episode  237 score -125.4 avg score -259.6\n",
            "episode  238 score -117.6 avg score -255.2\n",
            "episode  239 score -361.2 avg score -256.5\n",
            "episode  240 score -119.1 avg score -256.5\n",
            "episode  241 score -358.7 avg score -258.9\n",
            "episode  242 score -602.0 avg score -263.6\n",
            "episode  243 score -361.1 avg score -266.0\n",
            "episode  244 score -0.3 avg score -263.6\n",
            "episode  245 score -123.0 avg score -262.4\n",
            "episode  246 score -124.1 avg score -261.3\n",
            "episode  247 score -576.3 avg score -264.6\n",
            "episode  248 score -125.5 avg score -263.4\n",
            "episode  249 score -516.4 avg score -265.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TD3 \n",
        "TD3 stands for Twin Delayed DDPG"
      ],
      "metadata": {
        "id": "s9zqNkvinyEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "  def __init__(self, max_size, input_shape, n_actions):\n",
        "    self.mem_size =max_size\n",
        "    self.mem_cntr =0\n",
        "    self.state_memory =np.zeros((self.mem_size, *input_shape))\n",
        "    self.new_state_memory =np.zeros((self.mem_size, *input_shape))\n",
        "    self.action_memory =np.zeros((self.mem_size, n_actions))\n",
        "    self.reward_memory =np.zeros(self.mem_size)\n",
        "    self.terminal_memory =np.zeros(self.mem_size, dtype=np.bool)\n",
        "\n",
        "  def store_transition(self, state, action, reward, state_, done):\n",
        "    index =self.mem_cntr % self.mem_size\n",
        "\n",
        "    self.state_memory[index] =state\n",
        "    self.new_state_memory[index] =state_\n",
        "    self.action_memory[index] =action\n",
        "    self.reward_memory[index] =reward\n",
        "    self.terminal_memory[index] =done\n",
        "\n",
        "    self.mem_cntr +=1\n",
        "\n",
        "  def sample_buffer(self, batch_size):\n",
        "    max_mem =min(self.mem_cntr, self.mem_size)\n",
        "    batch =np.random.choice(max_mem, batch_size)\n",
        "\n",
        "    states =self.state_memory[batch]\n",
        "    new_state =self.new_state_memory[batch]\n",
        "    actions =self.action_memory[batch]\n",
        "    rewards =self.reward_memory[batch]\n",
        "    dones =self.terminal_memory[batch]\n",
        "\n",
        "    return states, actions, rewards, new_state, dones"
      ],
      "metadata": {
        "id": "w_BaOdW0w76o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CriticNetwork(keras.Model):\n",
        "    def __init__(self, fc1_dims, fc2_dims, name, chkpt_dir='tmp/td3'):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.model_name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_td3')\n",
        "\n",
        "        self.fc1 = Dense(self.fc1_dims, activation='relu')\n",
        "        self.fc2 = Dense(self.fc2_dims, activation='relu')\n",
        "        self.q = Dense(1, activation=None)\n",
        "\n",
        "    def call(self, state, action):\n",
        "        q1_action_value = self.fc1(tensorflow.concat([state, action], axis=1))\n",
        "        q1_action_value = self.fc2(q1_action_value)\n",
        "\n",
        "        q = self.q(q1_action_value)\n",
        "\n",
        "        return q\n",
        "\n",
        "\n",
        "class ActorNetwork(keras.Model):\n",
        "    def __init__(self, fc1_dims, fc2_dims, n_actions, name,\n",
        "                 chkpt_dir='tmp/td3'):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "        self.model_name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_td3')\n",
        "\n",
        "        self.fc1 = Dense(self.fc1_dims, activation='relu')\n",
        "        self.fc2 = Dense(self.fc2_dims, activation='relu')\n",
        "        self.mu = Dense(self.n_actions, activation='tanh')\n",
        "\n",
        "    def call(self, state):\n",
        "        prob = self.fc1(state)\n",
        "        prob = self.fc2(prob)\n",
        "\n",
        "        mu = self.mu(prob)\n",
        "\n",
        "        return "
      ],
      "metadata": {
        "id": "0jwLH-Ipn9ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, alpha, beta, input_dims, tau, env,\n",
        "                 gamma=0.99, update_actor_interval=2, warmup=1000,\n",
        "                 n_actions=2, max_size=1000000, layer1_size=400,\n",
        "                 layer2_size=300, batch_size=100, noise=0.1):\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.max_action = env.action_space.high[0]\n",
        "        self.min_action = env.action_space.low[0]\n",
        "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
        "        self.batch_size = batch_size\n",
        "        self.learn_step_cntr = 0\n",
        "        self.time_step = 0\n",
        "        self.warmup = warmup\n",
        "        self.n_actions = n_actions\n",
        "        self.update_actor_iter = update_actor_interval\n",
        "\n",
        "        self.actor = ActorNetwork(layer1_size, layer2_size,\n",
        "                                  n_actions=n_actions, name='actor')\n",
        "\n",
        "        self.critic_1 = CriticNetwork(layer1_size, layer2_size,\n",
        "                                      name='critic_1')\n",
        "        self.critic_2 = CriticNetwork(layer1_size, layer2_size,\n",
        "                                      name='critic_2')\n",
        "\n",
        "        self.target_actor = ActorNetwork(layer1_size, layer2_size,\n",
        "                                         n_actions=n_actions,\n",
        "                                         name='target_actor')\n",
        "        self.target_critic_1 = CriticNetwork(layer1_size, layer2_size,\n",
        "                                             name='target_critic_1')\n",
        "        self.target_critic_2 = CriticNetwork(layer1_size, layer2_size,\n",
        "                                             name='target_critic_2')\n",
        "\n",
        "        self.actor.compile(optimizer=Adam(learning_rate=alpha), loss='mean')\n",
        "        self.critic_1.compile(optimizer=Adam(learning_rate=beta),\n",
        "                              loss='mean_squared_error')\n",
        "        self.critic_2.compile(optimizer=Adam(learning_rate=beta),\n",
        "                              loss='mean_squared_error')\n",
        "\n",
        "        self.target_actor.compile(optimizer=Adam(learning_rate=alpha),\n",
        "                                  loss='mean')\n",
        "        self.target_critic_1.compile(optimizer=Adam(learning_rate=beta),\n",
        "                                     loss='mean_squared_error')\n",
        "        self.target_critic_2.compile(optimizer=Adam(learning_rate=beta),\n",
        "                                     loss='mean_squared_error')\n",
        "\n",
        "        self.noise = noise\n",
        "        self.update_network_parameters(tau=1)\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        if self.time_step < self.warmup:\n",
        "            mu = np.random.normal(scale=self.noise, size=(self.n_actions,))\n",
        "        else:\n",
        "            state = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
        "            # returns a batch size of 1, want a scalar array\n",
        "            mu = self.actor(state)[0]\n",
        "        mu_prime = mu + np.random.normal(scale=self.noise)\n",
        "\n",
        "        mu_prime = tf.clip_by_value(mu_prime, self.min_action, self.max_action)\n",
        "        self.time_step += 1\n",
        "\n",
        "        return mu_prime\n",
        "\n",
        "    def remember(self, state, action, reward, new_state, done):\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.mem_cntr < self.batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, new_states, dones = \\\n",
        "            self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
        "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
        "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "        new_states = tf.convert_to_tensor(new_states, dtype=tf.float32)\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            target_actions = self.target_actor(new_states)\n",
        "            target_actions = target_actions + \\\n",
        "                tf.clip_by_value(np.random.normal(scale=0.2), -0.5, 0.5)\n",
        "\n",
        "            target_actions = tf.clip_by_value(target_actions, self.min_action,\n",
        "                                              self.max_action)\n",
        "\n",
        "            q1_ = self.target_critic_1(new_states, target_actions)\n",
        "            q2_ = self.target_critic_2(new_states, target_actions)\n",
        "\n",
        "            q1 = tf.squeeze(self.critic_1(states, actions), 1)\n",
        "            q2 = tf.squeeze(self.critic_2(states, actions), 1)\n",
        "\n",
        "            # shape is [batch_size, 1], want to collapse to [batch_size]\n",
        "            q1_ = tf.squeeze(q1_, 1)\n",
        "            q2_ = tf.squeeze(q2_, 1)\n",
        "\n",
        "            critic_value_ = tf.math.minimum(q1_, q2_)\n",
        "            # in tf2 only integer scalar arrays can be used as indices\n",
        "            # and eager exection doesn't support assignment, so we can't do\n",
        "            # q1_[dones] = 0.0\n",
        "            target = rewards + self.gamma*critic_value_*(1-dones)\n",
        "            critic_1_loss = keras.losses.MSE(target, q1)\n",
        "            critic_2_loss = keras.losses.MSE(target, q2)\n",
        "\n",
        "        critic_1_gradient = tape.gradient(critic_1_loss,\n",
        "                                          self.critic_1.trainable_variables)\n",
        "        critic_2_gradient = tape.gradient(critic_2_loss,\n",
        "                                          self.critic_2.trainable_variables)\n",
        "\n",
        "        self.critic_1.optimizer.apply_gradients(\n",
        "                   zip(critic_1_gradient, self.critic_1.trainable_variables))\n",
        "        self.critic_2.optimizer.apply_gradients(\n",
        "                   zip(critic_2_gradient, self.critic_2.trainable_variables))\n",
        "\n",
        "        self.learn_step_cntr += 1\n",
        "\n",
        "        if self.learn_step_cntr % self.update_actor_iter != 0:\n",
        "            return\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            new_actions = self.actor(states)\n",
        "            critic_1_value = self.critic_1(states, new_actions)\n",
        "            actor_loss = -tf.math.reduce_mean(critic_1_value)\n",
        "\n",
        "        actor_gradient = tape.gradient(actor_loss,\n",
        "                                       self.actor.trainable_variables)\n",
        "        self.actor.optimizer.apply_gradients(\n",
        "                        zip(actor_gradient, self.actor.trainable_variables))\n",
        "\n",
        "        self.update_network_parameters()\n",
        "\n",
        "    def update_network_parameters(self, tau=None):\n",
        "        if tau is None:\n",
        "            tau = self.tau\n",
        "\n",
        "        weights = []\n",
        "        targets = self.target_actor.weights\n",
        "        for i, weight in enumerate(self.actor.weights):\n",
        "            weights.append(weight * tau + targets[i]*(1-tau))\n",
        "\n",
        "        self.target_actor.set_weights(weights)\n",
        "\n",
        "        weights = []\n",
        "        targets = self.target_critic_1.weights\n",
        "        for i, weight in enumerate(self.critic_1.weights):\n",
        "            weights.append(weight * tau + targets[i]*(1-tau))\n",
        "\n",
        "        self.target_critic_1.set_weights(weights)\n",
        "\n",
        "        weights = []\n",
        "        targets = self.target_critic_2.weights\n",
        "        for i, weight in enumerate(self.critic_2.weights):\n",
        "            weights.append(weight * tau + targets[i]*(1-tau))\n",
        "\n",
        "        self.target_critic_2.set_weights(weights)\n",
        "\n",
        "    def save_models(self):\n",
        "        print('... saving models ...')\n",
        "        self.actor.save_weights(self.actor.checkpoint_file)\n",
        "        self.critic_1.save_weights(self.critic_1.checkpoint_file)\n",
        "        self.critic_2.save_weights(self.critic_2.checkpoint_file)\n",
        "        self.target_actor.save_weights(self.target_actor.checkpoint_file)\n",
        "        self.target_critic_1.save_weights(self.target_critic_1.checkpoint_file)\n",
        "        self.target_critic_2.save_weights(self.target_critic_2.checkpoint_file)\n",
        "\n",
        "    def load_models(self):\n",
        "        print('... loading models ...')\n",
        "        self.actor.load_weights(self.actor.checkpoint_file)\n",
        "        self.critic_1.load_weights(self.critic_1.checkpoint_file)\n",
        "        self.critic_2.load_weights(self.critic_2.checkpoint_file)\n",
        "        self.target_actor.load_weights(self.target_actor.checkpoint_file)\n",
        "        self.target_critic_1.load_weights(self.target_critic_1.checkpoint_file)\n",
        "        self.target_critic_2.load_weights(self.target_critic_2.checkpoint_file)"
      ],
      "metadata": {
        "id": "EEWTkOX4n9Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  env = gym.make('BipedalWalker-v3')\n",
        "  agent = Agent(alpha=0.001, beta=0.001,\n",
        "          input_dims=env.observation_space.shape, tau=0.005,\n",
        "          env=env, batch_size=100, layer1_size=400, layer2_size=300,\n",
        "          n_actions=env.action_space.shape[0])\n",
        "  n_games = 1000\n",
        "  filename = 'tmp/td3' + 'walker_' + str(n_games) + '_games.png'\n",
        "\n",
        "  best_score = env.reward_range[0]\n",
        "  score_history = []\n",
        "\n",
        "  #agent.load_models()\n",
        "\n",
        "  for i in range(n_games):\n",
        "      observation = env.reset()\n",
        "      done = False\n",
        "      score = 0\n",
        "      while not done:\n",
        "          action = agent.choose_action(observation)\n",
        "          observation_, reward, done, info = env.step(action)\n",
        "          agent.remember(observation, action, reward, observation_, done)\n",
        "          agent.learn()\n",
        "          score += reward\n",
        "          observation = observation_\n",
        "      score_history.append(score)\n",
        "      avg_score = np.mean(score_history[-100:])\n",
        "\n",
        "      if avg_score > best_score:\n",
        "          best_score = avg_score\n",
        "          agent.save_models()\n",
        "      else:\n",
        "        pass\n",
        "\n",
        "      print('episode ', i, 'score %.1f' % score,\n",
        "              'average score %.1f' % avg_score)\n",
        "\n",
        "  x = [i+1 for i in range(n_games)]\n",
        "  plot_learning_curve(x, score_history, filename)"
      ],
      "metadata": {
        "id": "3aaQyKmvn9Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "n2MXgoKrn9Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e3fKaD8Ln9Px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NQgmJQM6n9Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ds1yWO2xn9Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mIkW_7odn9Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "myGxKSD0n9bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cESmsNcxn9dY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "f2dY0hhRn9hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y_IATJDRn9ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Fe5miuNCn9nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7iw6_rKQn9p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "615M-WvTn9t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1tX4ABH3n9wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GDxRav_un90U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5czOqUCRZ933"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}